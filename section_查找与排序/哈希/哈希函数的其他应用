



哈希函数的应用
    注：大数据的题有一大部分都可以用hash函数来解决，将大问题转换成小问题
    负载均衡
        经典服务器抗压结构
            - 一个前端服务器，假设有128台后端服务器,对于每一个请求，求hash值，mod128,将请求送到对应的
                后端服务器上。（负载均衡）
            - 如果要增减机器，这种结构就有问题，相当于hash表扩容，要对所有数据进行迁移
        一致性哈希，解决增删机器的问题
            - 目的就是又能负载均衡，又能增减机器
            - 把hash函数的output域,比如说0~pow(2,64)-1
            - 将后台机器的特有信息，比如说IP，对其求hash函数，打到环上
            - 如果要新增数据，将数据打到环上，顺时针，找到对应后台机器
                后台机器hash值排序数组放到前端服务器上，前端服务器就能使用二分找到对应服务器
            - 新增一台服务器m4，比如哈希值说在m2和m3之间，仅需将m2到m4的数据迁移到m4
            - 删除一台服务器,原来是m1,m2,m3,删除m2,将m2所有的数据放到m3上
        进一步扩展：
            问题：hash函数只有在数据量特别大的时候，才很均匀，很小的时候很难保证均匀
            - 虚拟节点技术
                -对于每台机器，给其造比如说1000个虚拟节点，这些节点打到环上
                -每台机器的虚拟节点负责的数据由真实机器来负责
                -这样，机器的数据增大了，hash就均匀了
    
    打印大文件中所有重复的字符串
        说明：大文件有100T,里面字符串是无序
        方法：
            假设有1000台机器，将大文件中每行计算hash code mod 1000,就能将文件分配到1000台机器
            这样，重复文本都会分配到同一台机器上，且不同种的字符串会均匀分布到一台机器上。
            如果还是太大，还可以继续分，就变成了正常打印重复的字符串问题
    布隆过滤器
        解决的问题:
            爬虫去重问题和黑名问题
            url黑名单问题
                - 如果有100亿url黑名单，这些黑名单是不给访问的，每个url64位
                - 需要一个函数，对于每个url，返回true或者false，其是否属于该黑名单
                - 如果使用hashset,则需要640G内存来存储这个hash表
                
                - 用k个哈希表，一个input，将其对应的k个位置都设为1
                - 存储结构采用bit的0,1存储
        过程：
            准备一个有m个bit位的数组
            准备K个hash函数
            生成黑名单
                对于一个url,分别进入K个哈希函数 mod m, 将其相应位置描黑,就说一个url进到了该数组了
            查找url是否在黑名单
                url,分别进入K个哈希函数 mod m, 如果全黑，则认为在黑名单中，否则不在
        关于失误率
            hash函数k的个数越多，失误率越小
            数组的空间越大，失误率越小
            失误类型是，宁可杀错，不可放过的类型 
            参数如何确定
                所需空间计算公式m = -1 * (n * lnP) / (ln2)^2
                    注：n是样本量，P是目标概率， m是数组大小(比特)
                所需哈希函数个数
                    k=ln2*m/n
                m,k确定之后，真实失误率
                    P = (1-e^(-(n*k/m)))
                例子：假设n = 100亿， P=万分之1，m大概是1310亿比特，也就是大约22.3g。需要哈希函数13个，真实失误率十万分之6。
            